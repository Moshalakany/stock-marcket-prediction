{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import argparse\n",
    "import logging\n",
    "from enhanced_sentiment_model import EnhancedStockSentimentAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('sentiment_evaluation.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load the combined news dataset\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): Path to the combined CSV file\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Loaded dataframe\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading dataset from {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        logger.info(f\"Loaded {len(df)} records\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading dataset: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def take_random_sample(df, sample_size=1000, random_seed=42):\n",
    "    \"\"\"\n",
    "    Take a random sample from the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    sample_size (int): Size of the sample to take\n",
    "    random_seed (int): Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Sample dataframe\n",
    "    \"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        logger.error(\"No data to sample\")\n",
    "        return None\n",
    "    \n",
    "    # Adjust sample size if larger than dataset\n",
    "    if sample_size > len(df):\n",
    "        logger.warning(f\"Sample size {sample_size} is larger than dataset size {len(df)}. Using full dataset.\")\n",
    "        return df\n",
    "    \n",
    "    logger.info(f\"Taking random sample of {sample_size} records\")\n",
    "    \n",
    "    # Take random sample\n",
    "    sample = df.sample(n=sample_size, random_state=random_seed)\n",
    "    \n",
    "    return sample\n",
    "\n",
    "def evaluate_model_on_sample(analyzer, sample_df, output_dir):\n",
    "    \"\"\"\n",
    "    Evaluate the sentiment model on a sample dataset\n",
    "    \n",
    "    Parameters:\n",
    "    analyzer (EnhancedStockSentimentAnalyzer): Initialized sentiment analyzer\n",
    "    sample_df (pd.DataFrame): Sample dataframe\n",
    "    output_dir (str): Directory to save results\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Results dataframe and metrics dictionary\n",
    "    \"\"\"\n",
    "    if sample_df is None or len(sample_df) == 0:\n",
    "        logger.error(\"No sample data to evaluate\")\n",
    "        return None, None\n",
    "    \n",
    "    # Check if 'title' column exists\n",
    "    if 'title' not in sample_df.columns:\n",
    "        logger.error(\"Dataset doesn't contain 'title' column\")\n",
    "        return None, None\n",
    "    \n",
    "    # Check if we have ground truth for evaluation\n",
    "    has_ground_truth = 'sentiment_class' in sample_df.columns\n",
    "    \n",
    "    logger.info(f\"Evaluating model on {len(sample_df)} samples (ground truth available: {has_ground_truth})\")\n",
    "    \n",
    "    # Get list of texts to classify\n",
    "    texts = sample_df['title'].tolist()\n",
    "    \n",
    "    # Predict sentiment for each text\n",
    "    predictions = []\n",
    "    logger.info(\"Classifying texts... This may take a while.\")\n",
    "    \n",
    "    # Process in batches to show progress\n",
    "    batch_size = 100\n",
    "    num_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(texts))\n",
    "        batch_texts = texts[start_idx:end_idx]\n",
    "        \n",
    "        logger.info(f\"Processing batch {i+1}/{num_batches} ({start_idx+1}-{end_idx} of {len(texts)})\")\n",
    "        batch_predictions = [analyzer.predict_sentiment(text, return_scores=True) for text in batch_texts]\n",
    "        predictions.extend(batch_predictions)\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = sample_df.copy()\n",
    "    results_df['predicted_sentiment'] = [p['sentiment'] for p in predictions]\n",
    "    results_df['confidence'] = [p['confidence'] for p in predictions]\n",
    "    \n",
    "    # Add score columns for each sentiment\n",
    "    sentiment_classes = ['negative', 'neutral', 'positive']\n",
    "    for sentiment in sentiment_classes:\n",
    "        results_df[f'score_{sentiment}'] = [p['scores'].get(sentiment, 0) for p in predictions]\n",
    "    \n",
    "    # Save results\n",
    "    results_path = os.path.join(output_dir, 'sentiment_evaluation_results.csv')\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "    logger.info(f\"Results saved to {results_path}\")\n",
    "    \n",
    "    # Evaluate if ground truth is available\n",
    "    metrics = {}\n",
    "    if has_ground_truth:\n",
    "        # Map sentiment classes to ensure consistent format\n",
    "        sentiment_map = {\n",
    "            'positive': 'positive',\n",
    "            'negative': 'negative', \n",
    "            'neutral': 'neutral',\n",
    "            'pos': 'positive',\n",
    "            'neg': 'negative',\n",
    "            'neu': 'neutral'\n",
    "        }\n",
    "        \n",
    "        # Normalize ground truth labels\n",
    "        results_df['sentiment_class'] = results_df['sentiment_class'].str.lower()\n",
    "        results_df['sentiment_class'] = results_df['sentiment_class'].map(\n",
    "            lambda x: sentiment_map.get(x, 'neutral') if isinstance(x, str) else 'neutral'\n",
    "        )\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(results_df['sentiment_class'], results_df['predicted_sentiment'])\n",
    "        logger.info(f\"Model accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Generate classification report\n",
    "        report = classification_report(\n",
    "            results_df['sentiment_class'],\n",
    "            results_df['predicted_sentiment'],\n",
    "            target_names=sentiment_classes,\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Log classification report\n",
    "        logger.info(\"\\nClassification Report:\")\n",
    "        logger.info(classification_report(\n",
    "            results_df['sentiment_class'],\n",
    "            results_df['predicted_sentiment'],\n",
    "            target_names=sentiment_classes\n",
    "        ))\n",
    "        \n",
    "        # Generate confusion matrix\n",
    "        cm = confusion_matrix(\n",
    "            results_df['sentiment_class'], \n",
    "            results_df['predicted_sentiment'],\n",
    "            labels=sentiment_classes\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "    \n",
    "    return results_df, metrics\n",
    "\n",
    "def plot_results(results_df, metrics, output_dir):\n",
    "    \"\"\"\n",
    "    Plot evaluation results\n",
    "    \n",
    "    Parameters:\n",
    "    results_df (pd.DataFrame): Results dataframe\n",
    "    metrics (dict): Evaluation metrics\n",
    "    output_dir (str): Directory to save plots\n",
    "    \"\"\"\n",
    "    if results_df is None:\n",
    "        logger.error(\"No results to plot\")\n",
    "        return\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # 1. Plot distribution of predicted sentiments\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.countplot(x='predicted_sentiment', data=results_df, order=['negative', 'neutral', 'positive'])\n",
    "    plt.title('Distribution of Predicted Sentiments')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # 2. Plot confidence distribution\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.histplot(results_df['confidence'], bins=20, kde=True)\n",
    "    plt.title('Distribution of Prediction Confidence')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    \n",
    "    # 3. Plot ground truth vs predicted if available\n",
    "    has_ground_truth = 'sentiment_class' in results_df.columns\n",
    "    \n",
    "    if has_ground_truth:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        ground_truth_counts = results_df['sentiment_class'].value_counts().reindex(['negative', 'neutral', 'positive'], fill_value=0)\n",
    "        predicted_counts = results_df['predicted_sentiment'].value_counts().reindex(['negative', 'neutral', 'positive'], fill_value=0)\n",
    "        \n",
    "        width = 0.35\n",
    "        x = np.arange(len(ground_truth_counts.index))\n",
    "        \n",
    "        plt.bar(x - width/2, ground_truth_counts.values, width, label='Ground Truth')\n",
    "        plt.bar(x + width/2, predicted_counts.values, width, label='Predicted')\n",
    "        plt.xlabel('Sentiment')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Ground Truth vs Predicted Sentiment')\n",
    "        plt.xticks(x, ground_truth_counts.index, rotation=45)\n",
    "        plt.legend()\n",
    "        \n",
    "        # 4. Plot confusion matrix\n",
    "        if 'confusion_matrix' in metrics:\n",
    "            plt.subplot(2, 2, 4)\n",
    "            cm = metrics['confusion_matrix']\n",
    "            sns.heatmap(\n",
    "                cm, \n",
    "                annot=True, \n",
    "                fmt='d', \n",
    "                cmap='Blues',\n",
    "                xticklabels=['negative', 'neutral', 'positive'],\n",
    "                yticklabels=['negative', 'neutral', 'positive']\n",
    "            )\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.title('Confusion Matrix')\n",
    "    else:\n",
    "        # If no ground truth, plot distribution of scores\n",
    "        plt.subplot(2, 2, 3)\n",
    "        avg_scores = {\n",
    "            'negative': results_df['score_negative'].mean(),\n",
    "            'neutral': results_df['score_neutral'].mean(),\n",
    "            'positive': results_df['score_positive'].mean()\n",
    "        }\n",
    "        sns.barplot(x=list(avg_scores.keys()), y=list(avg_scores.values()))\n",
    "        plt.title('Average Sentiment Scores')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # 4. Plot top 10 most confident predictions\n",
    "        plt.subplot(2, 2, 4)\n",
    "        top10 = results_df.sort_values('confidence', ascending=False).head(10)\n",
    "        sns.barplot(x='confidence', y='predicted_sentiment', data=top10, orient='h')\n",
    "        plt.title('Top 10 Most Confident Predictions')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(os.path.join(output_dir, 'sentiment_evaluation_plots.png'))\n",
    "    logger.info(f\"Plots saved to {output_dir}/sentiment_evaluation_plots.png\")\n",
    "    \n",
    "    # If we have ground truth, also save the classification report as an image\n",
    "    if has_ground_truth and 'classification_report' in metrics:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        report = metrics['classification_report']\n",
    "        \n",
    "        # Create table-like visualization\n",
    "        plt.axis('off')\n",
    "        \n",
    "        header = ['Sentiment', 'Precision', 'Recall', 'F1-score', 'Support']\n",
    "        cell_text = []\n",
    "        \n",
    "        for sentiment in ['negative', 'neutral', 'positive']:\n",
    "            row = [\n",
    "                sentiment,\n",
    "                f\"{report[sentiment]['precision']:.2f}\",\n",
    "                f\"{report[sentiment]['recall']:.2f}\",\n",
    "                f\"{report[sentiment]['f1-score']:.2f}\",\n",
    "                f\"{report[sentiment]['support']}\"\n",
    "            ]\n",
    "            cell_text.append(row)\n",
    "        \n",
    "        # Add accuracy row\n",
    "        accuracy = report['accuracy']\n",
    "        cell_text.append(['accuracy', '', '', f\"{accuracy:.2f}\", f\"{report['macro avg']['support']}\"])\n",
    "        \n",
    "        table = plt.table(\n",
    "            cellText=cell_text,\n",
    "            colLabels=header,\n",
    "            loc='center',\n",
    "            cellLoc='center',\n",
    "            colWidths=[0.15, 0.15, 0.15, 0.15, 0.15]\n",
    "        )\n",
    "        \n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(12)\n",
    "        table.scale(1.2, 1.8)\n",
    "        \n",
    "        plt.title('Classification Report', fontsize=16, pad=20)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the classification report\n",
    "        plt.savefig(os.path.join(output_dir, 'classification_report.png'))\n",
    "        logger.info(f\"Classification report saved to {output_dir}/classification_report.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data_path DATA_PATH]\n",
      "                             [--model_dir MODEL_DIR] [--output_dir OUTPUT_DIR]\n",
      "                             [--sample_size SAMPLE_SIZE]\n",
      "                             [--random_seed RANDOM_SEED]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\ahmed\\AppData\\Roaming\\jupyter\\runtime\\kernel-v3f8a5501b5c58aa7558289c5b1c8672c831063d9f.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Evaluate stock news sentiment model on a random sample')\n",
    "    \n",
    "    parser.add_argument('--data_path', type=str, \n",
    "                        default=r'E:\\Stock Market GP\\Reddit\\news\\sentiment_model_output\\enhanced_hybrid_model\\combined_stock_news.csv',\n",
    "                        help='Path to the combined stock news CSV file')\n",
    "    \n",
    "    parser.add_argument('--model_dir', type=str, \n",
    "                        default=r'E:\\Stock Market GP\\Reddit\\news\\sentiment_model_output\\enhanced_hybrid_model',\n",
    "                        help='Directory containing the trained sentiment model')\n",
    "    \n",
    "    parser.add_argument('--output_dir', type=str, \n",
    "                        default=r'E:\\Stock Market GP\\Reddit\\news\\sentiment_evaluation_results',\n",
    "                        help='Directory to save evaluation results')\n",
    "    \n",
    "    parser.add_argument('--sample_size', type=int, default=1000,\n",
    "                        help='Number of random news samples to evaluate')\n",
    "    \n",
    "    parser.add_argument('--random_seed', type=int, default=42,\n",
    "                        help='Random seed for reproducibility')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    \n",
    "    # 1. Load the dataset\n",
    "    df = load_dataset(args.data_path)\n",
    "    if df is None:\n",
    "        logger.error(\"Failed to load dataset. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Take a random sample\n",
    "    sample_df = take_random_sample(df, args.sample_size, args.random_seed)\n",
    "    if sample_df is None:\n",
    "        logger.error(\"Failed to create sample. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # 3. Initialize the analyzer\n",
    "    analyzer = EnhancedStockSentimentAnalyzer(\n",
    "        data_dir=os.path.dirname(args.data_path),\n",
    "        output_dir=args.model_dir\n",
    "    )\n",
    "    \n",
    "    # 4. Evaluate the model on the sample\n",
    "    results_df, metrics = evaluate_model_on_sample(analyzer, sample_df, args.output_dir)\n",
    "    if results_df is None:\n",
    "        logger.error(\"Evaluation failed. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # 5. Plot and save results\n",
    "    plot_results(results_df, metrics, args.output_dir)\n",
    "    \n",
    "    logger.info(\"Evaluation completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
