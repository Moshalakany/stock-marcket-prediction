{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Dense, Dropout, Concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import logging\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedStockSentimentAnalyzer:\n",
    "    def __init__(self, data_dir, output_dir):\n",
    "        \"\"\"\n",
    "        Initialize the Enhanced Stock Sentiment Analyzer\n",
    "        \n",
    "        Parameters:\n",
    "        data_dir (str): Directory containing CSV files with news data\n",
    "        output_dir (str): Directory to save processed data and models\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.combined_data = None\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.max_len = 150\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            \n",
    "        # Download NLTK resources\n",
    "        try:\n",
    "            nltk.data.find('tokenizers/punkt')\n",
    "            nltk.data.find('corpora/stopwords')\n",
    "            nltk.data.find('corpora/wordnet')\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "            nltk.download('stopwords')\n",
    "            nltk.download('wordnet')\n",
    "    \n",
    "    def load_and_combine_data(self):\n",
    "        \"\"\"\n",
    "        Load all CSV files from data_dir and combine them into one DataFrame\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading data from {self.data_dir}\")\n",
    "        all_files = [f for f in os.listdir(self.data_dir) if f.endswith('.csv')]\n",
    "        \n",
    "        if not all_files:\n",
    "            logger.error(f\"No CSV files found in {self.data_dir}\")\n",
    "            return False\n",
    "            \n",
    "        dataframes = []\n",
    "        \n",
    "        for file in all_files:\n",
    "            try:\n",
    "                file_path = os.path.join(self.data_dir, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Skip empty files\n",
    "                if df.empty:\n",
    "                    logger.warning(f\"Skipping empty file: {file}\")\n",
    "                    continue\n",
    "                \n",
    "                # Check if required columns are present\n",
    "                required_columns = ['title', 'sentiment_class', 'sentiment_score']\n",
    "                if not all(col in df.columns for col in required_columns):\n",
    "                    logger.warning(f\"Skipping file {file} due to missing required columns\")\n",
    "                    continue\n",
    "                \n",
    "                # Add ticker symbol (filename without extension)\n",
    "                ticker = os.path.splitext(file)[0]\n",
    "                df['ticker'] = ticker\n",
    "                \n",
    "                dataframes.append(df)\n",
    "                logger.info(f\"Loaded {file} with {len(df)} records\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading {file}: {str(e)}\")\n",
    "                \n",
    "        if not dataframes:\n",
    "            logger.error(\"No dataframes were successfully loaded\")\n",
    "            return False\n",
    "            \n",
    "        self.combined_data = pd.concat(dataframes, ignore_index=True)\n",
    "        logger.info(f\"Combined {len(dataframes)} files into dataset with {len(self.combined_data)} records\")\n",
    "        \n",
    "        # Save combined data\n",
    "        combined_path = os.path.join(self.output_dir, 'combined_stock_news.csv')\n",
    "        self.combined_data.to_csv(combined_path, index=False)\n",
    "        logger.info(f\"Saved combined data to {combined_path}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the text data and prepare for modeling\n",
    "        \"\"\"\n",
    "        if self.combined_data is None or len(self.combined_data) == 0:\n",
    "            logger.error(\"No data to preprocess. Run load_and_combine_data first.\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(\"Starting data preprocessing...\")\n",
    "        \n",
    "        # Drop any rows with missing titles\n",
    "        self.combined_data = self.combined_data.dropna(subset=['title'])\n",
    "        \n",
    "        # Ensure sentiment_class is properly formatted\n",
    "        self.combined_data['sentiment_class'] = self.combined_data['sentiment_class'].str.lower()\n",
    "        \n",
    "        # Map sentiment classes to ensure consistent labeling\n",
    "        sentiment_map = {\n",
    "            'positive': 'positive',\n",
    "            'negative': 'negative',\n",
    "            'neutral': 'neutral',\n",
    "            'pos': 'positive',\n",
    "            'neg': 'negative',\n",
    "            'neu': 'neutral'\n",
    "        }\n",
    "        \n",
    "        self.combined_data['sentiment_class'] = self.combined_data['sentiment_class'].map(\n",
    "            lambda x: sentiment_map.get(x, 'neutral') if isinstance(x, str) else 'neutral'\n",
    "        )\n",
    "        \n",
    "        # Clean text\n",
    "        logger.info(\"Cleaning text data...\")\n",
    "        self.combined_data['cleaned_title'] = self.combined_data['title'].apply(self.clean_text)\n",
    "        \n",
    "        # Encode sentiment classes\n",
    "        logger.info(\"Encoding sentiment classes...\")\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        label_encoder = LabelEncoder()\n",
    "        self.combined_data['sentiment_encoded'] = label_encoder.fit_transform(self.combined_data['sentiment_class'])\n",
    "        \n",
    "        # Save the label encoder for later use\n",
    "        with open(os.path.join(self.output_dir, 'label_encoder.pkl'), 'wb') as f:\n",
    "            pickle.dump(label_encoder, f)\n",
    "        \n",
    "        # Store class mappings\n",
    "        self.class_mappings = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "        logger.info(f\"Class mappings: {self.class_mappings}\")\n",
    "        \n",
    "        # Add sentiment polarity from score where available\n",
    "        if 'sentiment_score' in self.combined_data.columns:\n",
    "            self.combined_data['sentiment_polarity'] = self.combined_data['sentiment_score'].apply(\n",
    "                lambda x: (x - 0.5) * 2 if not pd.isna(x) else 0\n",
    "            )\n",
    "        \n",
    "        # Save preprocessed data\n",
    "        preprocessed_path = os.path.join(self.output_dir, 'preprocessed_stock_news.csv')\n",
    "        self.combined_data.to_csv(preprocessed_path, index=False)\n",
    "        logger.info(f\"Saved preprocessed data to {preprocessed_path}\")\n",
    "        \n",
    "        # Display data summary\n",
    "        logger.info(f\"Data summary after preprocessing:\")\n",
    "        logger.info(f\"Total records: {len(self.combined_data)}\")\n",
    "        logger.info(f\"Sentiment distribution: {self.combined_data['sentiment_class'].value_counts()}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean and normalize text data\n",
    "        \n",
    "        Parameters:\n",
    "        text (str): Text to clean\n",
    "        \n",
    "        Returns:\n",
    "        str: Cleaned text\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\d+', ' ', text)\n",
    "        # Remove extra whitespaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Tokenize\n",
    "        word_tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [self.lemmatizer.lemmatize(word) for word in word_tokens if word not in stop_words]\n",
    "        \n",
    "        return ' '.join(words)\n",
    "    \n",
    "    def prepare_training_data(self, max_words=15000, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"\n",
    "        Prepare the data for training the model\n",
    "        \n",
    "        Parameters:\n",
    "        max_words (int): Maximum number of words for the tokenizer\n",
    "        test_size (float): Proportion of data to use for testing\n",
    "        val_size (float): Proportion of training data to use for validation\n",
    "        \n",
    "        Returns:\n",
    "        tuple: Training, validation, and test data splits and metadata\n",
    "        \"\"\"\n",
    "        if self.combined_data is None:\n",
    "            logger.error(\"No data to prepare. Run preprocess_data first.\")\n",
    "            return None\n",
    "            \n",
    "        logger.info(\"Preparing training data...\")\n",
    "        \n",
    "        # Tokenize text\n",
    "        logger.info(f\"Tokenizing with vocabulary size of {max_words}...\")\n",
    "        tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "        tokenizer.fit_on_texts(self.combined_data['cleaned_title'])\n",
    "        \n",
    "        # Save tokenizer\n",
    "        with open(os.path.join(self.output_dir, 'tokenizer.pkl'), 'wb') as f:\n",
    "            pickle.dump(tokenizer, f)\n",
    "        \n",
    "        # Convert text to sequences\n",
    "        sequences = tokenizer.texts_to_sequences(self.combined_data['cleaned_title'])\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=self.max_len, padding='post', truncating='post')\n",
    "        \n",
    "        # Get targets\n",
    "        targets = self.combined_data['sentiment_encoded']\n",
    "        \n",
    "        # Convert to one-hot encoding for multi-class classification\n",
    "        targets_categorical = tf.keras.utils.to_categorical(targets)\n",
    "        \n",
    "        # Split data\n",
    "        logger.info(f\"Splitting data with test_size={test_size}, val_size={val_size}...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            padded_sequences, targets_categorical, test_size=test_size, random_state=42, stratify=targets_categorical\n",
    "        )\n",
    "        \n",
    "        # Further split training data into training and validation\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=val_size/(1-test_size), random_state=42, stratify=y_train\n",
    "        )\n",
    "        \n",
    "        # Save tokenizer and other metadata for later use\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = min(max_words, len(tokenizer.word_index) + 1)\n",
    "        \n",
    "        logger.info(f\"Training data shape: {X_train.shape}\")\n",
    "        logger.info(f\"Validation data shape: {X_val.shape}\")\n",
    "        logger.info(f\"Test data shape: {X_test.shape}\")\n",
    "        logger.info(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    def build_model(self, model_type='hybrid', embedding_dim=300):\n",
    "        \"\"\"\n",
    "        Build an enhanced deep learning model for sentiment analysis\n",
    "        \n",
    "        Parameters:\n",
    "        model_type (str): Type of model architecture ('lstm', 'cnn', 'hybrid')\n",
    "        embedding_dim (int): Dimensionality of embeddings\n",
    "        \n",
    "        Returns:\n",
    "        model: Compiled Keras model\n",
    "        \"\"\"\n",
    "        logger.info(f\"Building {model_type.upper()} model...\")\n",
    "        \n",
    "        input_layer = Input(shape=(self.max_len,))\n",
    "        \n",
    "        # Embedding layer\n",
    "        embedding_layer = Embedding(\n",
    "            input_dim=self.vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=self.max_len\n",
    "        )(input_layer)\n",
    "        \n",
    "        if model_type == 'lstm':\n",
    "            # LSTM-based model\n",
    "            x = Dropout(0.2)(embedding_layer)\n",
    "            x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Bidirectional(LSTM(64))(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(64, activation='relu')(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            \n",
    "        elif model_type == 'cnn':\n",
    "            # CNN-based model\n",
    "            conv1 = Conv1D(filters=128, kernel_size=3, activation='relu')(embedding_layer)\n",
    "            pool1 = GlobalMaxPooling1D()(conv1)\n",
    "            \n",
    "            conv2 = Conv1D(filters=128, kernel_size=4, activation='relu')(embedding_layer)\n",
    "            pool2 = GlobalMaxPooling1D()(conv2)\n",
    "            \n",
    "            conv3 = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
    "            pool3 = GlobalMaxPooling1D()(conv3)\n",
    "            \n",
    "            x = Concatenate()([pool1, pool2, pool3])\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(64, activation='relu')(x)\n",
    "            x = Dropout(0.2)(x)\n",
    "            \n",
    "        else:  # hybrid model (default)\n",
    "            # LSTM branch\n",
    "            lstm_branch = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n",
    "            lstm_branch = Dropout(0.2)(lstm_branch)\n",
    "            lstm_branch = Bidirectional(LSTM(64))(lstm_branch)\n",
    "            \n",
    "            # CNN branch - multiple filter sizes\n",
    "            conv1 = Conv1D(filters=128, kernel_size=3, activation='relu')(embedding_layer)\n",
    "            pool1 = GlobalMaxPooling1D()(conv1)\n",
    "            \n",
    "            conv2 = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding_layer)\n",
    "            pool2 = GlobalMaxPooling1D()(conv2)\n",
    "            \n",
    "            # Combine branches\n",
    "            x = Concatenate()([lstm_branch, pool1, pool2])\n",
    "            x = Dropout(0.2)(x)\n",
    "            x = Dense(128, activation='relu')(x)\n",
    "            x = Dropout(0.3)(x)\n",
    "        \n",
    "        # Output layer - 3 classes (positive, negative, neutral)\n",
    "        output_layer = Dense(3, activation='softmax')(x)\n",
    "        \n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Print model summary\n",
    "        model.summary(print_fn=logger.info)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def train_model(self, X_train, y_train, X_val, y_val, batch_size=32, epochs=15):\n",
    "        \"\"\"\n",
    "        Train the sentiment analysis model\n",
    "        \n",
    "        Parameters:\n",
    "        X_train: Training features\n",
    "        y_train: Training targets\n",
    "        X_val: Validation features\n",
    "        y_val: Validation targets\n",
    "        batch_size (int): Batch size\n",
    "        epochs (int): Maximum number of epochs\n",
    "        \n",
    "        Returns:\n",
    "        history: Training history\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            logger.error(\"Model not built. Call build_model() first.\")\n",
    "            return None\n",
    "            \n",
    "        logger.info(f\"Training model with batch_size={batch_size}, epochs={epochs}...\")\n",
    "        \n",
    "        # Setup callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=3,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                filepath=os.path.join(self.output_dir, 'best_model.keras'),\n",
    "                monitor='val_accuracy',\n",
    "                mode='max',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=2,\n",
    "                min_lr=0.0001,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Save model and training history\n",
    "        self.model.save(os.path.join(self.output_dir, 'final_model.keras'))\n",
    "        \n",
    "        # Plot training history\n",
    "        self.plot_training_history(history)\n",
    "        \n",
    "        return history\n",
    "        \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the trained model on test data\n",
    "        \n",
    "        Parameters:\n",
    "        X_test: Test features\n",
    "        y_test: Test targets\n",
    "        \n",
    "        Returns:\n",
    "        dict: Evaluation metrics\n",
    "        \"\"\"\n",
    "        # Try to load best model if available\n",
    "        best_model_path = os.path.join(self.output_dir, 'best_model.keras')\n",
    "        if os.path.exists(best_model_path):\n",
    "            logger.info(\"Loading best model for evaluation...\")\n",
    "            self.model = load_model(best_model_path)\n",
    "        \n",
    "        if self.model is None:\n",
    "            logger.error(\"No model available for evaluation\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(\"Evaluating model on test data...\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        loss, accuracy = self.model.evaluate(X_test, y_test, verbose=0)\n",
    "        logger.info(f\"Test loss: {loss:.4f}\")\n",
    "        logger.info(f\"Test accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Generate predictions\n",
    "        y_pred_probs = self.model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        y_true = np.argmax(y_test, axis=1)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        report = classification_report(\n",
    "            y_true, \n",
    "            y_pred, \n",
    "            target_names=['negative', 'neutral', 'positive'],\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Log classification report\n",
    "        logger.info(\"\\nClassification Report:\")\n",
    "        logger.info(classification_report(y_true, y_pred, target_names=['negative', 'neutral', 'positive']))\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        self.plot_confusion_matrix(cm, ['negative', 'neutral', 'positive'])\n",
    "        \n",
    "        # Create evaluation metrics dictionary\n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'loss': loss,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_training_history(self, history):\n",
    "        \"\"\"\n",
    "        Plot and save training history\n",
    "        \n",
    "        Parameters:\n",
    "        history: Model training history\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Plot accuracy\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "        \n",
    "        # Plot loss\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "        \n",
    "        # Plot learning rate if available\n",
    "        if 'lr' in history.history:\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.plot(history.history['lr'])\n",
    "            plt.title('Learning Rate')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'training_history.png'))\n",
    "        logger.info(f\"Training history plot saved to {self.output_dir}/training_history.png\")\n",
    "        \n",
    "    def plot_confusion_matrix(self, cm, class_names):\n",
    "        \"\"\"\n",
    "        Plot and save confusion matrix\n",
    "        \n",
    "        Parameters:\n",
    "        cm: Confusion matrix\n",
    "        class_names: List of class names\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        sns.heatmap(\n",
    "            cm, \n",
    "            annot=True, \n",
    "            fmt='d', \n",
    "            cmap='Blues',\n",
    "            xticklabels=class_names, \n",
    "            yticklabels=class_names\n",
    "        )\n",
    "        \n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(os.path.join(self.output_dir, 'confusion_matrix.png'))\n",
    "        logger.info(f\"Confusion matrix saved to {self.output_dir}/confusion_matrix.png\")\n",
    "    \n",
    "    def predict_sentiment(self, text, return_scores=False):\n",
    "        \"\"\"\n",
    "        Predict sentiment for new text\n",
    "        \n",
    "        Parameters:\n",
    "        text (str or list): Text or list of texts to analyze\n",
    "        return_scores (bool): Whether to return confidence scores\n",
    "        \n",
    "        Returns:\n",
    "        list or dict: Sentiment predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            # Try to load model\n",
    "            model_path = os.path.join(self.output_dir, 'best_model.keras')\n",
    "            if os.path.exists(model_path):\n",
    "                self.model = load_model(model_path)\n",
    "            else:\n",
    "                logger.error(\"No model available for prediction\")\n",
    "                return None\n",
    "        \n",
    "        if self.tokenizer is None:\n",
    "            # Try to load tokenizer\n",
    "            tokenizer_path = os.path.join(self.output_dir, 'tokenizer.pkl')\n",
    "            if os.path.exists(tokenizer_path):\n",
    "                with open(tokenizer_path, 'rb') as f:\n",
    "                    self.tokenizer = pickle.load(f)\n",
    "            else:\n",
    "                logger.error(\"No tokenizer available for prediction\")\n",
    "                return None\n",
    "        \n",
    "        # Handle single text or list of texts\n",
    "        single_text = False\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "            single_text = True\n",
    "        \n",
    "        # Clean and preprocess texts\n",
    "        cleaned_texts = [self.clean_text(t) for t in text]\n",
    "        \n",
    "        # Convert to sequences\n",
    "        sequences = self.tokenizer.texts_to_sequences(cleaned_texts)\n",
    "        padded_sequences = pad_sequences(sequences, maxlen=self.max_len, padding='post', truncating='post')\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.model.predict(padded_sequences)\n",
    "        \n",
    "        # Process results\n",
    "        results = []\n",
    "        sentiment_classes = ['negative', 'neutral', 'positive']\n",
    "        \n",
    "        for i, pred in enumerate(predictions):\n",
    "            sentiment_idx = np.argmax(pred)\n",
    "            sentiment = sentiment_classes[sentiment_idx]\n",
    "            \n",
    "            result = {'text': text[i], 'sentiment': sentiment}\n",
    "            \n",
    "            if return_scores:\n",
    "                result['confidence'] = float(pred[sentiment_idx])\n",
    "                result['scores'] = {sentiment_classes[j]: float(score) for j, score in enumerate(pred)}\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        # Return single result if input was a single text\n",
    "        if single_text:\n",
    "            return results[0]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_pipeline(self, model_type='hybrid', train_model=True, epochs=15):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline from data loading to model evaluation\n",
    "        \n",
    "        Parameters:\n",
    "        model_type (str): Type of model architecture ('lstm', 'cnn', 'hybrid')\n",
    "        train_model (bool): Whether to train the model or just prepare data\n",
    "        epochs (int): Number of epochs for training\n",
    "        \n",
    "        Returns:\n",
    "        dict: Evaluation metrics if model is trained and evaluated\n",
    "        \"\"\"\n",
    "        # 1. Load and combine data\n",
    "        if not self.load_and_combine_data():\n",
    "            logger.error(\"Failed to load and combine data\")\n",
    "            return None\n",
    "        \n",
    "        # 2. Preprocess data\n",
    "        if not self.preprocess_data():\n",
    "            logger.error(\"Failed to preprocess data\")\n",
    "            return None\n",
    "        \n",
    "        # 3. Prepare training data\n",
    "        data = self.prepare_training_data()\n",
    "        if data is None:\n",
    "            logger.error(\"Failed to prepare training data\")\n",
    "            return None\n",
    "        \n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = data\n",
    "        \n",
    "        if not train_model:\n",
    "            logger.info(\"Data preparation complete. Skipping model training as requested.\")\n",
    "            return {\n",
    "                'X_train': X_train, 'y_train': y_train,\n",
    "                'X_val': X_val, 'y_val': y_val,\n",
    "                'X_test': X_test, 'y_test': y_test\n",
    "            }\n",
    "        \n",
    "        # 4. Build model\n",
    "        self.model = self.build_model(model_type=model_type)\n",
    "        \n",
    "        # 5. Train model\n",
    "        history = self.train_model(X_train, y_train, X_val, y_val, epochs=epochs)\n",
    "        if history is None:\n",
    "            logger.error(\"Failed to train model\")\n",
    "            return None\n",
    "        \n",
    "        # 6. Evaluate model\n",
    "        metrics = self.evaluate_model(X_test, y_test)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage when run as a script\n",
    "if __name__ == \"__main__\":\n",
    "    # Set paths\n",
    "    data_directory = \"/e:/Stock Market GP/Reddit/news/sentiment_data\"\n",
    "    output_directory = \"/e:/Stock Market GP/Reddit/news/enhanced_sentiment_model_output\"\n",
    "    \n",
    "    # Initialize analyzer\n",
    "    analyzer = EnhancedStockSentimentAnalyzer(data_directory, output_directory)\n",
    "    \n",
    "    # Run the full pipeline with hybrid model (CNN + LSTM)\n",
    "    metrics = analyzer.run_pipeline(model_type='hybrid', train_model=True, epochs=15)\n",
    "    \n",
    "    if metrics:\n",
    "        logger.info(f\"Model training complete. Final accuracy: {metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        # Example prediction\n",
    "        sample_texts = [\n",
    "            \"Company reports record profits exceeding expectations\",\n",
    "            \"Stock plummets after poor quarterly results\",\n",
    "            \"Company announces new CEO appointment\"\n",
    "        ]\n",
    "        \n",
    "        for text in sample_texts:\n",
    "            prediction = analyzer.predict_sentiment(text, return_scores=True)\n",
    "            logger.info(f\"Text: {text}\")\n",
    "            logger.info(f\"Sentiment: {prediction['sentiment']} (confidence: {prediction['confidence']:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
